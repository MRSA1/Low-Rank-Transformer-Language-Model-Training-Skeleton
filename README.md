# Low-Rank-Transformer-Language-Model-Training-Skeleton
Efficient LLM trainer using low-rank attention &amp; ALiBi. Train capable models on consumer hardware. 80% fewer parameters, FP8 support, progressive training. Perfect for research &amp; startups. Pretrain + automatic chat fine-tuning.
